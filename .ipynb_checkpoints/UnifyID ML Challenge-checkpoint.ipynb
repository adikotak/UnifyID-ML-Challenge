{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Aditya Kotak\n",
    "For the UnifyID ML Challenge\n",
    "Implements a kNN classification to make predictions for which activity is being done.\n",
    "Data from: http://ps.ewi.utwente.nl/Datasets.php\n",
    "\"\"\"\n",
    "import csv, random, math, operator\n",
    "trainSet, testSet, predictions = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to implement a 70-30 train-test split rather than the 70-10-20 train-implmentation-test split\n",
    "because I wanted to only implement the kNN classificaiton. I know that kNN is not the most accurate classification\n",
    "method but I felt it was resaonble for this task, thus the 10 percent validation split was unecessary. \n",
    "If I was going to test more classification methods, I would use the that 10 percent dataset and run it through \n",
    "many classifications and then decide the best method and then use the 20 percent test set with that \n",
    "optimal classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k, split = 3, 0.7\n",
    "wristFileName, beltFileName, pocketFileName, armFileName = \"wrist.data.csv\", \"belt.data.csv\", \"pocket.data.csv\", \"arm.data.csv\"\n",
    "\n",
    "\"\"\"Functionality to load a CSV into an list of lists.\"\"\"\n",
    "def load(myfile, splitPercent, train=[] , test=[]):\n",
    "    with open(myfile, 'r') as mycsv:\n",
    "        iterator = csv.reader(mycsv)\n",
    "        next(iterator, None)\n",
    "        mydata = list(iterator)\n",
    "        for i in range(len(mydata)-1):\n",
    "            for j in range(0,10):\n",
    "                mydata[i][j] = float(mydata[i][j])\n",
    "            if random.random() < splitPercent:\n",
    "                train.append(mydata[i])\n",
    "            else:\n",
    "                test.append(mydata[i])\n",
    "\n",
    "\"\"\"Basis of the kNN algorithm. Calculates the 'distance' between two datapoints.\"\"\"\n",
    "def calcDist(dataSet1, dataSet2, mylen):\n",
    "    dist = 0\n",
    "    for i in range(mylen):\n",
    "        dist += pow((dataSet1[i] - dataSet2[i]), 2)\n",
    "    return math.sqrt(dist)\n",
    "\n",
    "\"\"\"Finds the k nearest neighbors by finding the k elements with the shortest distance.\"\"\"\n",
    "def findNeighbors(trainSet, test, k):\n",
    "    distances = []\n",
    "    testLen = len(test)-1\n",
    "    for i in range(len(trainSet)):\n",
    "        dist = calcDist(test, trainSet[i], testLen)\n",
    "        distances.append((trainSet[i], dist))\n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "\"\"\"Makes the prediction for the activity by using the k nearest neighbors\"\"\"\n",
    "def predict(neighbors):\n",
    "    rawPredictions = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        prediction = neighbors[i][-1]\n",
    "        if prediction in rawPredictions:\n",
    "            rawPredictions[prediction] += 1\n",
    "        else:\n",
    "            rawPredictions[prediction] = 1\n",
    "    sortedPredictions = sorted(rawPredictions.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedPredictions[0][0]\n",
    "\n",
    "\"\"\"Calculates how accurate the prediction is by cross referencing the actual test data set\"\"\"\n",
    "def calcAccuracy(test, predicts, nums):\n",
    "    accurate = 0\n",
    "    for i in range(nums):\n",
    "        if predicts[i] == test[i][-1]:\n",
    "            accurate += 1\n",
    "    return (accurate/float(nums)) * 100.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Runs the actual predictions with a given dataset fileName\"\"\"\n",
    "def run(fileName):\n",
    "    load(fileName, split, trainSet, testSet)\n",
    "    predictionNums = len(testSet)\n",
    "\n",
    "    for i in range(predictionNums):\n",
    "        neighbors = findNeighbors(trainSet, testSet[i], k)\n",
    "        output = predict(neighbors)\n",
    "        predictions.append(output)\n",
    "        print(\"predicted = \" + repr(output) + \", actual = \" + repr(testSet[i][-1]))\n",
    "    accuracy = calcAccuracy(testSet, predictions, predictionNums)\n",
    "    print('Accuracy: ' + repr(accuracy) + '%')\n",
    "\n",
    "\"\"\"Runs the wrist data\"\"\"\n",
    "def runWrist():\n",
    "    run(wristFileName)\n",
    "\n",
    "\"\"\"Runs the belt data\"\"\"\n",
    "def runBelt():\n",
    "    run(beltFileName)\n",
    "\n",
    "\"\"\"Runs the pocket data\"\"\"\n",
    "def runPocket():\n",
    "    run(pocketFileName)\n",
    "\n",
    "\"\"\"Runs the arm data\"\"\"\n",
    "def runArm():\n",
    "    run(armFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runBelt() # runs analysis on the Belt data. Change this command to work with the other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on runtime -- this is a faily inefficient implementation that takes some time to make predictions. This is due to the inherent functionality of kNN, where I have to loop through the entire dataset multiple times to generate a predcition. I approached solving this challenge not with the mentality of proving my prowess in Machine Learning (something I don't have) but rather my aptitude to learn quickly. \n",
    "\n",
    "I started this challenge on Saturday, 2/25 and finished it by Sunday, 2/26. In that process, I learned how to implement a Maching Learning classification from scratch and understand the nuances of ML. I hope my solution is seen as a testament of my ability to learn, an ability I am eager to put to the test at UnifyID this summer and through that process, still add value to the team. \n",
    "\n",
    "Best,\n",
    "Aditya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies from a sample size of 49 data points (limited due to time constraints)\n",
    "\n",
    "<img src=\"files/accuracies.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
